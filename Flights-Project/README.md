# Data Engineering Project: Data Pipeline and Visualization

This project demonstrates the creation of a scalable data pipeline using modern data engineering tools and technologies. The workflow involves extracting raw data, processing it for analytics, and creating insightful visualizations.

## Tools and Technologies Used
- **Apache Airflow**: Workflow orchestration and scheduling.
- **Apache Spark**: Distributed data processing and transformation.
- **Google Cloud Storage (GCS)**: Staging raw and processed data.
- **Google BigQuery**: Data warehousing for analytics.
- **Power BI**: Interactive data visualization and reporting.


## Project Overview

### Objective
The objective of this project is to build an end-to-end data pipeline to:
1. Extract raw data from a source.
2. Transform the data into a clean and usable format.
3. Store the processed data in a data warehouse.
4. Analyze the data using visualizations to derive business insights.

### Workflow Diagram

```plaintext
Source Data -> Extract (Airflow) -> Transform (Spark) -> Load to GCS -> BigQuery -> Power BI
```

# Report
![Report]()

